---
layout: post
title:  "集成学习"
date:   2019-01-03 10:43:45 +0800
categories: [machinelearning]
---

# 集成学习，以提高机器学习结果
集成方法如何工作：装袋，提升和堆叠

*通过组合多个模型，集成学习有助于提高机器学习效果。与单个模型相比，该方法允许产生更好的预测性能。这就是为什么集合方法在许多着名的机器学习竞赛中首先出现的原因，例如Netflix竞赛，KDD 2009和Kaggle。*
Statsbot团队想给你这个方法的优点，并采访了数据科学家Vadim Smolyakov，深入研究三个基本集成学习方法。

集成学习是一种元算法，它将多种机器学习技术组合成一个预测模型，以减少**方差**（套袋），**偏差**（增强）或**改进预测**（堆叠）。

集成方法可分为两类：
- 顺序集成方法，其中基本学习器是顺序生成的（例如AdaBoost）。
顺序方法的基本动机是**利用基础学习者之间的依赖关系**。通过称重先前错误标记的具有较高重量的示例，可以提高整体性能。
- 并行集成方法，其中基础学习器是并行生成的（例如随机森林）。
并行方法的基本动机是**利用基础学习者之间的独立性**，因为通过平均可以显着减少误差。

大多数集合方法使用单一基础学习算法来生成同类基础学习器，即相同类型的学习器，从而导致同质集成。
还有一些方法使用异构学习者，即不同类型的学习器，导致异构的集成。为了使集成方法比其任何单个更准确，基础学习器必须尽可能准确且尽可能多样化。

## 装袋法（bagging）
装袋法代表bootstrap聚合。减少估计方差的一种方法是将多个估计值平均在一起。例如，我们可以在数据的不同子集上训练M个不同的树（随机选择替换）并计算整体：

$$f(x) = 1/M\sum_{m=1}^{M}f_m(x)$$

装袋法使用自举采样来获得用于训练基础学习者的数据子集。为了汇总基础学习者的输出，装袋法使用投票进行分类并对回归进行平均。

我们可以在Iris数据集的分类环境中研究装袋法。我们可以选择两个基本估计器：决策树和k-NN分类器。图1显示了基本估计器的学习决策边界以及应用于Iris数据集的装袋集合。
```
精度：0.63（+/- 0.02）[决策树] 
精度：0.70（+/- 0.02）[K-NN] 
精度：0.64（+/- 0.01）[套袋树] 
精度：0.59（+/- 0.07）[套袋K-NN]
```
![图1](http://wenhao-public.oss-cn-hangzhou.aliyuncs.com/blogs/el-p1.png)

决策树显示轴的平行边界，而k = 1个最近邻点与数据点紧密配合。使用10个基础估计器训练Bagging集成模型，其具有0.8个子采样训练数据和0.8个子采样特征。

与k-NN装袋整体相比，决策树装袋整体实现了更高的精度。K-NN对训练样本的扰动不太敏感，因此它们被称为稳定学习器。

*结合稳定的学习器不太有利，因为不会有助于提高泛化性能。*

该图还显示了测试精度如何随着整体的大小而提高。基于交叉验证结果，我们可以看到准确度增加到大约10个基本估计值，然后就比较平稳。因此，对于Iris数据集，添加超过10的基本估计器仅增加计算复杂度，而没有准确性增益。

我们还可以看到套袋树集合的学习曲线。注意训练数据的平均误差为0.3，测试数据的U形误差曲线。训练和测试错误之间的最小差距发生在训练集大小的80％左右。

*常用的一类集成算法是随机森林算法。*

在**随机森林**中，整体中的每棵树都是从训练集中用替换（即自举样本）绘制的样本构建的。此外，不是使用所有特征，而是选择随机的特征子集，进一步随机化树。

![图2](http://wenhao-public.oss-cn-hangzhou.aliyuncs.com/blogs/el-p2.png)

因此，森林的偏差略有增加，但由于较少相关树的平均值，其方差减小，从而产生整体更好的模型。

在极端随机树算法中，随机性更进一步：分裂阈值是随机的。不是寻找最具辨别力的阈值，而是针对每个候选特征随机绘制阈值，并且挑选这些随机生成的阈值中的最佳阈值作为分裂规则。这通常允许更多地减少模型的方差，代价是偏差略微增加。

## 提升（boosting）
提升法是指一系列能够将弱学习器转变为强学习器的算法。提升的主要原则是拟合一系列弱学习者-模型仅比随机猜测稍微好一点，例如小决策树-加权版本的数据。对前几轮错误分类的样本给予了更多的重视。

然后通过加权多数投票（分类）或加权和（回归）来组合预测以产生最终预测。提升和委员会方法（例如装袋）之间的主要区别在于，基础学习器按照数据的加权版本按顺序进行训练。

下面的算法描述了最广泛使用的称为AdaBoost的增强算法， 它代表自适应增强。

![图3](http://wenhao-public.oss-cn-hangzhou.aliyuncs.com/blogs/el-p3.png)

我们看到第一个基本分类器y1(x)是使用全部相等的加权系数训练的。在随后的增强轮次中，对于被错误分类的数据点和针对正确分类的数据点减少的加权系数增加。

数量epsilon表示每个基本分类器的加权错误率。因此，加权系数α给予更准确的分类器更大的权重。

![图4](http://wenhao-public.oss-cn-hangzhou.aliyuncs.com/blogs/el-p4.png)

AdaBoost算法如上图所示。每个基础学习器由具有深度1的决策树组成，因此基于特征阈值对数据进行分类，该特征阈值将空间划分为由与其中一个轴平行的线性决策表面分隔的两个区域。该图还显示了测试精度如何随着整体的大小以及训练和测试数据的学习曲线而提高。

**梯度树提升**是对任意可微分损失函数的一种通用提升方法。它可以用于回归和分类问题。梯度提升以顺序方式构建模型。

$$F_m(x) = F_{m-1}(x)+γ_mh_m(x)$$

在每个阶段，选择决策树hm(x)以在给定当前模型Fm-1(x)的情况下最小化损失函数L：

$$F_m(x) = F_{m-1}(x)+argmin_h\sum_{i=1}^{n}L(y_i,F_{m-1}(x_i)+h(x_i))$$

回归和分类的算法在所使用的损失函数的类型上不同。

## 堆叠（stacking）
堆叠是一种集成学习技术，它通过元分类器或元回归器组合多个分类或回归模型。基础模型基于完整的训练集进行训练，然后将基础模型的输出作为特征训练元模型。

基本级别通常由不同的学习算法组成，因此堆叠集合通常是异构的。下面的算法总结了堆叠。

![图5](http://wenhao-public.oss-cn-hangzhou.aliyuncs.com/blogs/el-p5.png)
![图6](http://wenhao-public.oss-cn-hangzhou.aliyuncs.com/blogs/el-p6.png)

以下精度在上图的右上图中可视化：
```
精度：0.91（+/- 0.01）[KNN] 
精度：0.91（+/- 0.06）[随机森林] 
精度：0.92（+/- 0.03）[朴素贝叶斯] 
精度：0.95（+/- 0.03）[堆积分类器]
```
堆叠整体如上图所示。它由k-NN，随机森林和朴素贝叶斯基本分类器组成，其预测由逻辑回归组合为元分类器。我们可以看到堆叠分类器实现的决策边界的混合。该图还显示堆叠实现了比单个分类器更高的准确度，并且基于学习曲线，它没有显示过度拟合的迹象。

堆叠是赢得Kaggle数据科学竞赛的常用技术。例如，Otto集团产品分类挑战的第一名是由30多个模型的堆叠集合赢得的，其输出被用作三个元分类器的特征：XGBoost，神经网络和Adaboost。有关详细信息，请参阅以下[链接](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335)

## 代码
为了查看用于生成所有数字的代码，请查看以下[ipython笔记本](https://github.com/vsmolyakov/experiments_with_python/blob/master/chp01/ensemble_methods.ipynb)。
## 结论
除了本文研究的方法之外，通过训练多样化和准确的分类器，在深度学习中使用集合是很常见的。可以通过改变架构，超参数设置和训练技术来实现多样性。
集合方法在挑战数据集上创造记录性能方面非常成功，并且是Kaggle数据科学竞赛的最佳赢家之一。
## 推荐阅读
- Zhi-Hua Zhou, “Ensemble Methods: Foundations and Algorithms”, CRC Press, 2012
- L. Kuncheva, “Combining Pattern Classifiers: - Methods and Algorithms”, Wiley, 2004
- Kaggle Ensembling Guide
- Scikit Learn Ensemble Guide
- S. Rachka, MLxtend library
- Kaggle Winning Ensemble

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>